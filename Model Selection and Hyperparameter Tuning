{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BelayAbAb/AI-Powered-Credit-Scoring-Model-/blob/Refactor-Codebase-for-Modularity-and-Maintainability/Model%20Selection%20and%20Hyperparameter%20Tuning\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYV91hbKwP2J"
      },
      "source": [
        "Colab notebooks execute code on Google's cloud servers, meaning you can leverage the power of Google hardware, including [GPUs and TPUs](#using-accelerated-hardware), regardless of the power of your machine. All you need is a browser.\n",
        "\n",
        "For example, if you find yourself waiting for **pandas** code to finish running and want to go faster, you can switch to a GPU Runtime and use libraries like [RAPIDS cuDF](https://rapids.ai/cudf-pandas) that provide zero-code-change acceleration."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install necessary dependencies\n",
        "!pip install -q seaborn matplotlib scikit-learn gdown\n",
        "\n",
        "# Step 2: Import necessary libraries\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import gdown  # This will allow downloading from Google Drive\n",
        "\n",
        "# Step 3: Download the file using the shared Google Drive link\n",
        "\n",
        "# Shared link: https://drive.google.com/file/d/1OpwfxIO8aeDDsdSEgGrnyn1F6MFw4O6x/view?usp=sharing\n",
        "# Extract the file ID from the link (the ID is the part between /d/ and /view)\n",
        "file_id = '1OpwfxIO8aeDDsdSEgGrnyn1F6MFw4O6x'\n",
        "\n",
        "# Construct the download URL\n",
        "download_url = f'https://drive.google.com/uc?export=download&id={file_id}'\n",
        "\n",
        "# Download the file using gdown\n",
        "gdown.download(download_url, 'data.csv', quiet=False)\n",
        "\n",
        "# Step 4: Load the CSV file from the local directory\n",
        "df = pd.read_csv('data.csv')  # The file will be downloaded to the current directory\n",
        "df.dataframeName = 'data.csv'\n",
        "\n",
        "# Step 5: Check the shape of the data\n",
        "nRow, nCol = df.shape\n",
        "print(f'There are {nRow} rows and {nCol} columns in {df.dataframeName}')\n",
        "\n",
        "# Step 6: Take a quick look at the data\n",
        "print(df.head(5))\n",
        "\n",
        "# Step 7: Exploratory Data Analysis (EDA)\n",
        "\n",
        "# Filter out non-numeric columns for correlation and other numerical operations\n",
        "numeric_df = df.select_dtypes(include=[np.number])\n",
        "\n",
        "# Define output directory in Google Drive\n",
        "output_dir = '/content/drive/MyDrive/1fINHoR_jYkPkHB-7HPm1fxQqIxeFOKnR/EDA_Results/'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Distribution of numeric columns - Save the plot to Google Drive\n",
        "def plotPerColumnDistribution(df, nRows, nCols):\n",
        "    df.hist(figsize=(nRows, nCols))\n",
        "    plt.savefig(os.path.join(output_dir, 'distribution_plot.png'))\n",
        "    plt.close()\n",
        "\n",
        "plotPerColumnDistribution(numeric_df, 10, 5)\n",
        "\n",
        "# Correlation matrix of numeric columns - Save the plot to Google Drive\n",
        "def plotCorrelationMatrix(df, nRows):\n",
        "    corr = df.corr()\n",
        "    plt.figure(figsize=(nRows, nRows))\n",
        "    sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
        "    plt.savefig(os.path.join(output_dir, 'correlation_matrix.png'))\n",
        "    plt.close()\n",
        "\n",
        "plotCorrelationMatrix(numeric_df, 8)\n",
        "\n",
        "# Scatter and density plots (only for numeric columns) - Save the plot to Google Drive\n",
        "def plotScatterMatrix(df, nRows, nCols):\n",
        "    sns.pairplot(df, height=2.5)\n",
        "    plt.savefig(os.path.join(output_dir, 'scatter_matrix.png'))\n",
        "    plt.close()\n",
        "\n",
        "plotScatterMatrix(numeric_df, 12, 10)\n",
        "\n",
        "# Save the processed data as CSV to Google Drive\n",
        "output_csv_path = os.path.join(output_dir, 'processed_data.csv')\n",
        "df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "# Conclusion message\n",
        "print(f\"All output files have been saved to: {output_dir}\")\n",
        "print(\"This concludes the exploratory data analysis!\")\n"
      ],
      "metadata": {
        "id": "zE4NZl-ik9wh",
        "outputId": "267694c2-8116-4476-cf30-c0876946602a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1OpwfxIO8aeDDsdSEgGrnyn1F6MFw4O6x\n",
            "To: /content/data.csv\n",
            "100%|██████████| 17.4M/17.4M [00:00<00:00, 211MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 95662 rows and 16 columns in data.csv\n",
            "         TransactionId         BatchId       AccountId       SubscriptionId  \\\n",
            "0  TransactionId_76871   BatchId_36123  AccountId_3957   SubscriptionId_887   \n",
            "1  TransactionId_73770   BatchId_15642  AccountId_4841  SubscriptionId_3829   \n",
            "2  TransactionId_26203   BatchId_53941  AccountId_4229   SubscriptionId_222   \n",
            "3    TransactionId_380  BatchId_102363   AccountId_648  SubscriptionId_2185   \n",
            "4  TransactionId_28195   BatchId_38780  AccountId_4841  SubscriptionId_3829   \n",
            "\n",
            "        CustomerId CurrencyCode  CountryCode    ProviderId     ProductId  \\\n",
            "0  CustomerId_4406          UGX          256  ProviderId_6  ProductId_10   \n",
            "1  CustomerId_4406          UGX          256  ProviderId_4   ProductId_6   \n",
            "2  CustomerId_4683          UGX          256  ProviderId_6   ProductId_1   \n",
            "3   CustomerId_988          UGX          256  ProviderId_1  ProductId_21   \n",
            "4   CustomerId_988          UGX          256  ProviderId_4   ProductId_6   \n",
            "\n",
            "      ProductCategory    ChannelId   Amount  Value  TransactionStartTime  \\\n",
            "0             airtime  ChannelId_3   1000.0   1000  2018-11-15T02:18:49Z   \n",
            "1  financial_services  ChannelId_2    -20.0     20  2018-11-15T02:19:08Z   \n",
            "2             airtime  ChannelId_3    500.0    500  2018-11-15T02:44:21Z   \n",
            "3        utility_bill  ChannelId_3  20000.0  21800  2018-11-15T03:32:55Z   \n",
            "4  financial_services  ChannelId_2   -644.0    644  2018-11-15T03:34:21Z   \n",
            "\n",
            "   PricingStrategy  FraudResult  \n",
            "0                2            0  \n",
            "1                2            0  \n",
            "2                2            0  \n",
            "3                2            0  \n",
            "4                2            0  \n",
            "All output files have been saved to: /content/drive/MyDrive/1fINHoR_jYkPkHB-7HPm1fxQqIxeFOKnR/EDA_Results/\n",
            "This concludes the exploratory data analysis!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gdown\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "# Suppress specific warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Step 1: Download the dataset from Google Drive\n",
        "file_id = '1OpwfxIO8aeDDsdSEgGrnyn1F6MFw4O6x'\n",
        "download_url = f'https://drive.google.com/uc?export=download&id={file_id}'\n",
        "gdown.download(download_url, 'data.csv', quiet=False)\n",
        "\n",
        "# Step 2: Load the dataset\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Check for missing values and data types\n",
        "print(\"Data types:\\n\", data.dtypes)\n",
        "print(\"Missing values:\\n\", data.isnull().sum())\n",
        "\n",
        "# Step 3: Split the data into features (X) and target (y)\n",
        "# We assume 'FraudResult' is the target variable for binary classification\n",
        "X = data.drop(columns=['FraudResult', 'TransactionId', 'BatchId', 'AccountId', 'SubscriptionId', 'CustomerId', 'TransactionStartTime'])\n",
        "y = data['FraudResult']\n",
        "\n",
        "# Step 4: Split the data into training (70%) and testing (30%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Step 5: Define preprocessing steps for numerical and categorical features\n",
        "numerical_features = X_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(\"Numerical features:\", numerical_features)\n",
        "print(\"Categorical features:\", categorical_features)\n",
        "\n",
        "# Create a ColumnTransformer for preprocessing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[('num', StandardScaler(), numerical_features),\n",
        "                  ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Step 6: Define models with increased max_iter for Logistic Regression\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=5000),  # Increased max_iter\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier()\n",
        "}\n",
        "\n",
        "# Hyperparameter tuning parameters\n",
        "param_grid = {\n",
        "    'Logistic Regression': {\n",
        "        'model__C': [0.01, 0.1, 1, 10, 100],\n",
        "        'model__solver': ['liblinear', 'lbfgs', 'saga', 'newton-cg']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'model__n_estimators': [50, 100, 200],\n",
        "        'model__max_depth': [None, 10, 20, 30],\n",
        "        'model__min_samples_split': [2, 5, 10]\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'model__n_estimators': [50, 100, 200],\n",
        "        'model__learning_rate': [0.01, 0.1, 0.2],\n",
        "        'model__max_depth': [3, 5, 7]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Step 7: Train and evaluate each model\n",
        "results = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    # Create a pipeline with preprocessing and model\n",
        "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                ('model', model)])\n",
        "\n",
        "    # Perform hyperparameter tuning\n",
        "    if model_name == 'Logistic Regression':\n",
        "        grid_search = GridSearchCV(pipeline, param_grid[model_name], cv=5, scoring='f1', verbose=1)\n",
        "        grid_search.fit(X_train, y_train)\n",
        "    else:\n",
        "        grid_search = RandomizedSearchCV(pipeline, param_grid[model_name], n_iter=10, cv=5, scoring='f1', random_state=42)\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Store the best model and its score\n",
        "    best_model = grid_search.best_estimator_\n",
        "    results[model_name] = {\n",
        "        'best_model': best_model,\n",
        "        'best_score': grid_search.best_score_,\n",
        "    }\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    results[model_name]['accuracy'] = accuracy_score(y_test, y_pred)\n",
        "    results[model_name]['precision'] = precision_score(y_test, y_pred)\n",
        "    results[model_name]['recall'] = recall_score(y_test, y_pred)\n",
        "    results[model_name]['f1_score'] = f1_score(y_test, y_pred)\n",
        "    results[model_name]['roc_auc'] = roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1])\n",
        "\n",
        "    # Step 8: Confusion Matrix Visualization\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Not Fraud', 'Fraud'])\n",
        "\n",
        "    # Save confusion matrix plot\n",
        "    output_folder = '/content/drive/MyDrive/your_folder_path_here/'\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    cm_output_path = os.path.join(output_folder, f\"cm_{model_name}.jpg\")\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.title(f'Confusion Matrix for {model_name}')\n",
        "    plt.savefig(cm_output_path)\n",
        "    plt.close()\n",
        "\n",
        "# Step 9: Prepare data for plotting\n",
        "metric_names = ['Best CV Score', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']\n",
        "metric_values = {model: [results[model]['best_score'],\n",
        "                         results[model]['accuracy'],\n",
        "                         results[model]['precision'],\n",
        "                         results[model]['recall'],\n",
        "                         results[model]['f1_score'],\n",
        "                         results[model]['roc_auc']] for model in models.keys()}\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "width = 0.15  # Width of the bars\n",
        "x = np.arange(len(metric_names))  # the label locations\n",
        "\n",
        "for i, model in enumerate(models.keys()):\n",
        "    ax.bar(x + i * width, metric_values[model], width, label=model)\n",
        "\n",
        "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "ax.set_ylabel('Scores')\n",
        "ax.set_title('Model Training and Evaluation Metrics')\n",
        "ax.set_xticks(x + width / 2)\n",
        "ax.set_xticklabels(metric_names)\n",
        "ax.legend()\n",
        "\n",
        "# Save the plot as JPG in the specified local folder\n",
        "output_plot_path = os.path.join(output_folder, \"model_evaluation_metrics.jpg\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(output_plot_path)\n",
        "plt.close()\n",
        "\n",
        "# Step 10: Save the results to a CSV file\n",
        "output_results_path = os.path.join(output_folder, \"model_results.csv\")\n",
        "results_df = pd.DataFrame.from_dict(results, orient='index')\n",
        "results_df.to_csv(output_results_path)\n",
        "\n",
        "print(\"Model evaluation metrics and results saved successfully.\")\n"
      ],
      "metadata": {
        "id": "_h4G3y0Ms7YF",
        "outputId": "269653da-abb7-4d13-a169-c344cca0c6bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1OpwfxIO8aeDDsdSEgGrnyn1F6MFw4O6x\n",
            "To: /content/data.csv\n",
            "100%|██████████| 17.4M/17.4M [00:00<00:00, 110MB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data types:\n",
            " TransactionId            object\n",
            "BatchId                  object\n",
            "AccountId                object\n",
            "SubscriptionId           object\n",
            "CustomerId               object\n",
            "CurrencyCode             object\n",
            "CountryCode               int64\n",
            "ProviderId               object\n",
            "ProductId                object\n",
            "ProductCategory          object\n",
            "ChannelId                object\n",
            "Amount                  float64\n",
            "Value                     int64\n",
            "TransactionStartTime     object\n",
            "PricingStrategy           int64\n",
            "FraudResult               int64\n",
            "dtype: object\n",
            "Missing values:\n",
            " TransactionId           0\n",
            "BatchId                 0\n",
            "AccountId               0\n",
            "SubscriptionId          0\n",
            "CustomerId              0\n",
            "CurrencyCode            0\n",
            "CountryCode             0\n",
            "ProviderId              0\n",
            "ProductId               0\n",
            "ProductCategory         0\n",
            "ChannelId               0\n",
            "Amount                  0\n",
            "Value                   0\n",
            "TransactionStartTime    0\n",
            "PricingStrategy         0\n",
            "FraudResult             0\n",
            "dtype: int64\n",
            "Numerical features: ['CountryCode', 'Amount', 'Value', 'PricingStrategy']\n",
            "Categorical features: ['CurrencyCode', 'ProviderId', 'ProductId', 'ProductCategory', 'ChannelId']\n",
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Model evaluation metrics and results saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hocNkJbvBSPj",
        "outputId": "0ee734b4-e608-4e37-d196-7de7fcd0975a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'bin'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-77bcf73a68fd>\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mdf_cleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_cleaned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwoe_recency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mdf_cleaned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'WoE'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'WoE_Recency'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mdf_cleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_cleaned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwoe_frequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0mdf_cleaned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'WoE'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'WoE_Frequency'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mdf_cleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_cleaned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwoe_monetary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m  10830\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10832\u001b[0;31m         return merge(\n\u001b[0m\u001b[1;32m  10833\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10834\u001b[0m             \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    168\u001b[0m         )\n\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         op = _MergeOperation(\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1308\u001b[0m                         \u001b[0;31m#  the latter of which will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m                         \u001b[0mlk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m                         \u001b[0mleft_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1311\u001b[0m                         \u001b[0mjoin_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'bin'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}